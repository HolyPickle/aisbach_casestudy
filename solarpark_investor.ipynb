{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_full_names = {\n",
    "    \"Enviria\": \"ENVIRIA Energy Holding GmbH\",\n",
    "    \"Enrego\": \"ENREGO Energy GmbH\",\n",
    "    \"HIH Invest\": \"HIH Invest Real Estate Austria GmbH\",\n",
    "    \"Merkle\": \"Merkle Germany GmbH\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = {\n",
    "    \"Enviria\": \"https://enviria.energy/en/blog\",\n",
    "    \"Enrego\": \"https://enrego.de/en/about/\",\n",
    "    \"HIH Invest\": \"https://hih.de/en/media/press-release/\",\n",
    "    \"Merkle\": \"https://www.merkle.com/en/about-us/industry-expertise/financial-services.html\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame(companies.items(), columns=[\"company\", \"website\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company website parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviria: The company invests in solar parks.\n",
      "Enrego: The company invests in solar parks.\n",
      "HIH Invest: No clear indication of investment in solar parks found.\n",
      "Merkle: No clear indication of investment in solar parks found.\n"
     ]
    }
   ],
   "source": [
    "#Add column to df\n",
    "comp_df[\"invests in solar\"] = None\n",
    "\n",
    "#Load model\n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "for item in companies:\n",
    "    # Fetch the webpage\n",
    "    response = requests.get(companies[item])\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Convert the entire webpage text to lowercase for case-insensitive search\n",
    "        webpage_text = soup.get_text().lower()\n",
    "        # Process the text with spaCy\n",
    "        doc = model(webpage_text)\n",
    "        \n",
    "        # Define keywords and phrases indicating solar park investments\n",
    "        keywords = [\"solar park\", \"solar energy\", \"solar power\", \"solar investment\", \"solar park investment\", \"solarpark\", \"solarpark investment\"]\n",
    "        \n",
    "        # Search for the keywords in the processed text\n",
    "        invests_in_solar_park = any(phrase for phrase in keywords if phrase in doc.text.lower())\n",
    "        \n",
    "        if invests_in_solar_park:\n",
    "            comp_df.loc[comp_df[\"company\"] == item, \"invests in solar\"] = True\n",
    "            print(item + \": The company invests in solar parks.\")\n",
    "            \n",
    "        else:\n",
    "            comp_df.loc[comp_df[\"company\"] == item, \"invests in solar\"] = False\n",
    "            print(item + \": No clear indication of investment in solar parks found.\")\n",
    "    else:\n",
    "        \n",
    "        print(\"Failed to fetch the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.to_csv(\"results/company_site_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google News HTML Scraping\n",
    "* Google returns different results when requests used.\n",
    "* I even tried to replicate my cookies in my requests but nothing worked\n",
    "* So I come up with a different solution: Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=Enviria+solar&tbm=nws\n",
      "Articles for Enviria:\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape news articles from Google News\n",
    "def scrape_google_news(company):\n",
    "    search_urls = [f'https://www.google.com/search?q=\"{company}\"+solar&tbm=nws', \n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+investment&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solarpark&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+park&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+park+investment&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+energy&tbm=nws'\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+energy+investment&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+power&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+power+investment&tbm=nws'\n",
    "                   ]\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15\",\n",
    "        }\n",
    "    articles = []\n",
    "    for search_url in search_urls:\n",
    "        print(search_url)\n",
    "        time.sleep(5)  # Wait for 5 seconds before making the next request\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for item in soup.find_all('div', attrs={'class': \"MjjYud\"}):\n",
    "            print(\"adsad\")\n",
    "            title = item.find('div', attrs={'class': 'n0jPhd ynAwRc MBeuO nDgy9d'}).get_text()\n",
    "            desc = item.find('div', attrs={'class': 'GI74Re nDgy9d'}).get_text()\n",
    "            link = item.find('a', href=True)['href']\n",
    "            articles.append({\"title\": title, \"desc\": desc, \"link\": link})\n",
    "            print(f\"Title: {title}, Description: {desc}, Link: {link}\")\n",
    "    return articles\n",
    "\n",
    "# Gather articles for each company\n",
    "articles = {}\n",
    "for company in companies:\n",
    "    articles[company] = scrape_google_news(company)\n",
    "\n",
    "# Print out the gathered articles for inspection\n",
    "for company, arts in articles.items():\n",
    "    print(f\"Articles for {company}:\")\n",
    "    for art in arts:\n",
    "        print(f\"Title: {art['title']}, Link: {art['link']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "def start_driver():\n",
    "    # Set the path to the chromedriver executable\n",
    "    chromedriver_path = '/usr/local/bin/chromedriver'\n",
    "\n",
    "    # Start the WebDriver and load the page\n",
    "    service = Service(chromedriver_path)\n",
    "\n",
    "    # Start the WebDriver\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    \n",
    "    # Handle cookie consent\n",
    "    driver.get('https://www.google.com')\n",
    "\n",
    "    try:\n",
    "        # Wait for the cookie consent button to be clickable\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"L2AGLb\"]/div'))).click()\n",
    "        print(\"Cookie consent accepted.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error accepting cookies:\", e)\n",
    "\n",
    "    return driver \n",
    "\n",
    "    \n",
    "\n",
    "def close_driver(driver):\n",
    "    driver.quit()\n",
    "\n",
    "def scrape_with_selenium(company, driver, max_pages):\n",
    "    # Define the search URLs\n",
    "    search_urls = [f'https://www.google.com/search?q=\"{company}\"+solar&tbm=nws', \n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+investment&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solarpark&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+park&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+park+investment&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+energy&tbm=nws'\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+energy+investment&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+power&tbm=nws',\n",
    "                   f'https://www.google.com/search?q=\"{company}\"+solar+power+investment&tbm=nws'\n",
    "                   ]\n",
    "    articles = []\n",
    "    print(f\"Scraping articles for {company}...\")\n",
    "    # Loop through the search URLs\n",
    "    for search_url in search_urls:\n",
    "        driver.get(search_url)\n",
    "        current_page = 0\n",
    "        # Loop through the search results pages\n",
    "        while current_page < max_pages:\n",
    "            # Wait for the search results to load\n",
    "            parsed_articles = driver.find_elements(By.CSS_SELECTOR, 'div#rso > div >div>div>div')\n",
    "            # Parse the articles on the current page\n",
    "            for article_div in parsed_articles:\n",
    "                try:\n",
    "                    # Extract the article link\n",
    "                    link = article_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    # Extract the article details\n",
    "                    items = article_div.find_elements(By.CSS_SELECTOR, 'a>div>div>div')\n",
    "                    article = [item.text for item in items]\n",
    "                    # Check if the article has all the required details\n",
    "                    if len(article) >= 5:\n",
    "                        articles.append({\"source\": article[1], \"title\": article[2], \"desc\": article[3], \"date\": article[4], \"link\": link})\n",
    "                except Exception as e:\n",
    "                    print(\"Error parsing article:\", e)\n",
    "\n",
    "            # Try to find and click the \"Next\" button to go to the next page\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, '//*[@id=\"pnnext\"]')\n",
    "                if next_button:\n",
    "                    next_button.click()\n",
    "                    current_page += 1\n",
    "                else:\n",
    "                    break  # If \"Next\" button not found, exit the loop\n",
    "            except NoSuchElementException:\n",
    "                break  # If \"Next\" button not found, exit the loop\n",
    "    print(f\"Scraped {len(articles)} articles for {company}.\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent accepted.\n",
      "Scraping articles for Enviria...\n",
      "Scraped 152 articles for Enviria.\n",
      "Scraping articles for Enrego...\n",
      "Scraped 24 articles for Enrego.\n",
      "Scraping articles for HIH Invest...\n",
      "Scraped 49 articles for HIH Invest.\n",
      "Scraping articles for Merkle...\n",
      "Scraped 37 articles for Merkle.\n"
     ]
    }
   ],
   "source": [
    "driver = start_driver()\n",
    "\n",
    "articles = {}\n",
    "for company in companies:\n",
    "    articles[company] = scrape_with_selenium(company, driver=driver, max_pages=3)\n",
    "close_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert articles dictionary into a DataFrame\n",
    "articles_df = pd.DataFrame()\n",
    "\n",
    "for company, news in articles.items():\n",
    "    temp_df = pd.DataFrame(news)\n",
    "    temp_df['company'] = company  # Add a column for the company name if desired\n",
    "    temp_df = temp_df.reindex(columns=['company', 'source', 'title', 'desc', 'date', 'link'])\n",
    "    articles_df = pd.concat([articles_df, temp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df_unique = articles_df.drop_duplicates(subset=['company', 'title', 'desc'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enviria</td>\n",
       "      <td>EU-Startups</td>\n",
       "      <td>Frankfurt-based ENVIRIA secures €185 million t...</td>\n",
       "      <td>ENVIRIA, Germany's leading commercial and indu...</td>\n",
       "      <td>29 Feb 2024</td>\n",
       "      <td>https://www.eu-startups.com/2024/02/frankfurt-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enviria</td>\n",
       "      <td>Sifted</td>\n",
       "      <td>Solar panel startup Enviria secures $200m from...</td>\n",
       "      <td>Frankfurt-based Enviria has secured $200m in e...</td>\n",
       "      <td>29 Feb 2024</td>\n",
       "      <td>https://sifted.eu/articles/enviria-blackrock-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enviria</td>\n",
       "      <td>Tech.eu</td>\n",
       "      <td>Germany's Enviria targets commercial solar ene...</td>\n",
       "      <td>German solar startup Enviria raises over $200M...</td>\n",
       "      <td>29 Feb 2024</td>\n",
       "      <td>https://tech.eu/2024/02/29/germanys-enviria-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enviria</td>\n",
       "      <td>Renewables Now</td>\n",
       "      <td>Galileo sheds interest in Enviria to BlackRock...</td>\n",
       "      <td>Pan-European renewables developer Galileo Gree...</td>\n",
       "      <td>5 Mar 2024</td>\n",
       "      <td>https://renewablesnow.com/news/galileo-sheds-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enviria</td>\n",
       "      <td>pv magazine International</td>\n",
       "      <td>BlackRock invests $200 million in Enviria</td>\n",
       "      <td>BlackRock has invested €183 million ($200.2 mi...</td>\n",
       "      <td>11 Mar 2024</td>\n",
       "      <td>https://www.pv-magazine.com/2024/03/11/blackro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Merkle</td>\n",
       "      <td>Südkurier</td>\n",
       "      <td>(Anzeige) 10 Jahre Geba GmbH und ein Jahr Geba...</td>\n",
       "      <td>Rickenbach (psc) Die Firma GEBA GmbH mit Sitz ...</td>\n",
       "      <td>23 Mar 2019</td>\n",
       "      <td>https://www.suedkurier.de/region/hochrhein/ric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Merkle</td>\n",
       "      <td>The Merkle News</td>\n",
       "      <td>Will Mining Cryptocurrency in the Desert Using...</td>\n",
       "      <td>Mining Bitcoin or any other cryptocurrency is ...</td>\n",
       "      <td>5 Jul 2017</td>\n",
       "      <td>https://themerkle.com/will-mining-cryptocurren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Merkle</td>\n",
       "      <td>NOKZEIT</td>\n",
       "      <td>24. Dezember 2021 - Solarpark - „Made in Seckach“</td>\n",
       "      <td>Auf diesem Gelände soll der Solarpark entstehe...</td>\n",
       "      <td>24 Dec 2021</td>\n",
       "      <td>https://www.nokzeit.de/2021/12/24/solarpark-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Merkle</td>\n",
       "      <td>Investopedia</td>\n",
       "      <td>Is Solar-Powered Cryptocurrency Mining the Nex...</td>\n",
       "      <td>In the search to make cryptocurrency mining pr...</td>\n",
       "      <td>7 Nov 2017</td>\n",
       "      <td>https://www.investopedia.com/news/solarpowered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Merkle</td>\n",
       "      <td>The Merkle News</td>\n",
       "      <td>Envion – Mobile Crypto Miner Deploys to Lowest...</td>\n",
       "      <td>China's renewable energy sector wasted more th...</td>\n",
       "      <td>25 Dec 2017</td>\n",
       "      <td>https://themerkle.com/envion-mobile-crypto-min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     company                     source  \\\n",
       "0    Enviria                EU-Startups   \n",
       "1    Enviria                     Sifted   \n",
       "2    Enviria                    Tech.eu   \n",
       "3    Enviria             Renewables Now   \n",
       "4    Enviria  pv magazine International   \n",
       "..       ...                        ...   \n",
       "252   Merkle                  Südkurier   \n",
       "253   Merkle            The Merkle News   \n",
       "254   Merkle                    NOKZEIT   \n",
       "260   Merkle               Investopedia   \n",
       "261   Merkle            The Merkle News   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Frankfurt-based ENVIRIA secures €185 million t...   \n",
       "1    Solar panel startup Enviria secures $200m from...   \n",
       "2    Germany's Enviria targets commercial solar ene...   \n",
       "3    Galileo sheds interest in Enviria to BlackRock...   \n",
       "4            BlackRock invests $200 million in Enviria   \n",
       "..                                                 ...   \n",
       "252  (Anzeige) 10 Jahre Geba GmbH und ein Jahr Geba...   \n",
       "253  Will Mining Cryptocurrency in the Desert Using...   \n",
       "254  24. Dezember 2021 - Solarpark - „Made in Seckach“   \n",
       "260  Is Solar-Powered Cryptocurrency Mining the Nex...   \n",
       "261  Envion – Mobile Crypto Miner Deploys to Lowest...   \n",
       "\n",
       "                                                  desc         date  \\\n",
       "0    ENVIRIA, Germany's leading commercial and indu...  29 Feb 2024   \n",
       "1    Frankfurt-based Enviria has secured $200m in e...  29 Feb 2024   \n",
       "2    German solar startup Enviria raises over $200M...  29 Feb 2024   \n",
       "3    Pan-European renewables developer Galileo Gree...   5 Mar 2024   \n",
       "4    BlackRock has invested €183 million ($200.2 mi...  11 Mar 2024   \n",
       "..                                                 ...          ...   \n",
       "252  Rickenbach (psc) Die Firma GEBA GmbH mit Sitz ...  23 Mar 2019   \n",
       "253  Mining Bitcoin or any other cryptocurrency is ...   5 Jul 2017   \n",
       "254  Auf diesem Gelände soll der Solarpark entstehe...  24 Dec 2021   \n",
       "260  In the search to make cryptocurrency mining pr...   7 Nov 2017   \n",
       "261  China's renewable energy sector wasted more th...  25 Dec 2017   \n",
       "\n",
       "                                                  link  \n",
       "0    https://www.eu-startups.com/2024/02/frankfurt-...  \n",
       "1    https://sifted.eu/articles/enviria-blackrock-c...  \n",
       "2    https://tech.eu/2024/02/29/germanys-enviria-ta...  \n",
       "3    https://renewablesnow.com/news/galileo-sheds-i...  \n",
       "4    https://www.pv-magazine.com/2024/03/11/blackro...  \n",
       "..                                                 ...  \n",
       "252  https://www.suedkurier.de/region/hochrhein/ric...  \n",
       "253  https://themerkle.com/will-mining-cryptocurren...  \n",
       "254  https://www.nokzeit.de/2021/12/24/solarpark-ma...  \n",
       "260  https://www.investopedia.com/news/solarpowered...  \n",
       "261  https://themerkle.com/envion-mobile-crypto-min...  \n",
       "\n",
       "[125 rows x 6 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/4rcdnhdn5kj2w2l6mrnhw_nh0000gn/T/ipykernel_73020/2532103934.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  articles_df_unique['investing_in_solarparks'] = articles_df_unique.apply(lambda row: check_solar_investment(row['title'], row['desc']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "articles_df_unique = articles_df.drop_duplicates(subset=['company', 'title', 'desc'], inplace=False)\n",
    "\n",
    "keywords = [\n",
    "    'invests in solar park', 'investment in solar park', 'solar park investment', \n",
    "    'investing in solar park', 'solar park', 'solar energy project', \n",
    "    'solar power plant', 'renewable energy investment', 'solar project', 'solar energy investment',\n",
    "    'solar power', 'solarpark', 'investing in solar energy', 'investing in solar power'\n",
    "]\n",
    "\n",
    "# Function to check if the company is investing in solar parks\n",
    "def check_solar_investment(title, desc):\n",
    "    combined_text = f\"{title} {desc}\".lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword in combined_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column\n",
    "articles_df_unique['investing_in_solarparks'] = articles_df_unique.apply(lambda row: check_solar_investment(row['title'], row['desc']), axis=1)\n",
    "\n",
    "articles_df_unique.to_csv(\"results/rule_based_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company     investing_in_solarparks\n",
       "Enrego      False                      17\n",
       "            True                        7\n",
       "Enviria     False                      40\n",
       "            True                       14\n",
       "HIH Invest  False                      13\n",
       "            True                        2\n",
       "Merkle      False                      28\n",
       "            True                        4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_based_results = articles_df_unique.groupby(['company', 'investing_in_solarparks']).size()\n",
    "rule_based_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Based checking with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom patterns for solar park investments\n",
    "solar_patterns = [\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"park\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"project\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"energy\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"power\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"park\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"project\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"energy\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"power\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"initiative\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"farm\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"facility\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"installation\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"project\"}, {\"LOWER\": \"funding\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"energy\"}, {\"LOWER\": \"funding\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"power\"}, {\"LOWER\": \"funding\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"park\"}, {\"LOWER\": \"funding\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"initiative\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"farm\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"facility\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"installation\"}, {\"LOWER\": \"investment\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"energy\"}, {\"LOWER\": \"project\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"power\"}, {\"LOWER\": \"project\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"investment\"}, {\"LOWER\": \"fund\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"park\"}, {\"LOWER\": \"investment\"}, {\"LOWER\": \"fund\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"project\"}, {\"LOWER\": \"investment\"}, {\"LOWER\": \"fund\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"energy\"}, {\"LOWER\": \"investment\"}, {\"LOWER\": \"fund\"}]},\n",
    "    {\"label\": \"SOLAR_INVESTMENT\", \"pattern\": [{\"LOWER\": \"solar\"}, {\"LOWER\": \"power\"}, {\"LOWER\": \"investment\"}, {\"LOWER\": \"fund\"}]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/4rcdnhdn5kj2w2l6mrnhw_nh0000gn/T/ipykernel_73020/3552266276.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  articles_df_unique['investing_in_solarparks'] = articles_df_unique.apply(lambda row: extract_info_spacy(row['title'], row['desc']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "articles_df_unique = articles_df.drop_duplicates(subset=['company', 'title', 'desc'], inplace=False)\n",
    "\n",
    "# Function to extract relevant information using spaCy\n",
    "def extract_info_spacy(title, desc, solar_patterns=solar_patterns):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Add the custom patterns to the model\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(solar_patterns)\n",
    "    combined_text = f\"{title} {desc}\"\n",
    "    doc = nlp(combined_text)\n",
    "    investing_in_solarparks = False\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SOLAR_INVESTMENT\":\n",
    "            investing_in_solarparks = True\n",
    "    \n",
    "    return investing_in_solarparks\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column\n",
    "articles_df_unique['investing_in_solarparks'] = articles_df_unique.apply(lambda row: extract_info_spacy(row['title'], row['desc']), axis=1)\n",
    "\n",
    "articles_df_unique.to_csv(\"results/nlp_based_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company     investing_in_solarparks\n",
       "Enrego      False                      15\n",
       "            True                        9\n",
       "Enviria     False                      34\n",
       "            True                       20\n",
       "HIH Invest  False                      13\n",
       "            True                        2\n",
       "Merkle      False                      24\n",
       "            True                        8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_results = articles_df_unique.groupby(['company', 'investing_in_solarparks']).size()\n",
    "spacy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further parsing with NewsPaper\n",
    "* Using gathered links from Google News to fetch article data from the news sites\n",
    "* Extraction information with SpaCy\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/var/folders/b0/4rcdnhdn5kj2w2l6mrnhw_nh0000gn/T/ipykernel_73020/3479817003.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  articles_df_unique['detailed_content'] = articles_df_unique['link'].apply(fetch_and_parse)\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "articles_df_unique = articles_df.drop_duplicates(subset=['company', 'title', 'desc'], inplace=False)\n",
    "\n",
    "# Function to fetch and parse content using newspaper\n",
    "def fetch_and_parse(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "    \n",
    "\n",
    "\n",
    "# Add a new column to store the fetched content\n",
    "articles_df_unique['detailed_content'] = articles_df_unique['link'].apply(fetch_and_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Function to add the final content to the DataFrame\n",
    "# If the detailed content contains the the beginnig of description, use the detailed content\n",
    "def add_final_content(row):\n",
    "    desc = row['desc']\n",
    "    detailed_content = row['detailed_content']\n",
    "\n",
    "    if pd.isna(desc) or not isinstance(desc, str):\n",
    "        desc = \"\"\n",
    "    if pd.isna(detailed_content) or not isinstance(detailed_content, str):\n",
    "        detailed_content = \"\"\n",
    "    \n",
    "    desc = desc.lower()\n",
    "    detailed_content = detailed_content.lower()\n",
    "\n",
    "    key_terms = desc.split()\n",
    "    number_of_matches = 0\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_terms = [term for term in key_terms if term not in stop_words]\n",
    "    # Check if the key terms are present in the detailed content\n",
    "    for term in filtered_terms:\n",
    "        if term in detailed_content:\n",
    "            number_of_matches += 1\n",
    "        # If more than 8 key terms are found in the detailed content, return the detailed content\n",
    "        # I just used 8 as an arbitrary number, this can be adjusted based on the use case\n",
    "        if number_of_matches >= 8:\n",
    "            return detailed_content\n",
    "    return desc\n",
    "\n",
    "articles_df_unique['final_content'] = articles_df_unique.apply(add_final_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information using spaCy\n",
    "def extract_info(text, solar_patterns=solar_patterns):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(solar_patterns)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SOLAR_INVESTMENT\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Apply the extraction function to the final_content column\n",
    "articles_df_unique['investing_in_solarparks'] = articles_df_unique['final_content'].apply(extract_info)\n",
    "\n",
    "articles_df_unique.to_csv(\"results/newspaper_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company     investing_in_solarparks\n",
       "Enrego      False                       6\n",
       "            True                       18\n",
       "Enviria     False                      18\n",
       "            True                       36\n",
       "HIH Invest  False                      12\n",
       "            True                        3\n",
       "Merkle      False                      21\n",
       "            True                       11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaper_results = articles_df_unique.groupby(['company', 'investing_in_solarparks']).size()\n",
    "newspaper_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based results: \n",
      " company     investing_in_solarparks\n",
      "Enrego      False                      17\n",
      "            True                        7\n",
      "Enviria     False                      40\n",
      "            True                       14\n",
      "HIH Invest  False                      13\n",
      "            True                        2\n",
      "Merkle      False                      28\n",
      "            True                        4\n",
      "dtype: int64\n",
      "SpaCy-based results: \n",
      " company     investing_in_solarparks\n",
      "Enrego      False                      15\n",
      "            True                        9\n",
      "Enviria     False                      34\n",
      "            True                       20\n",
      "HIH Invest  False                      13\n",
      "            True                        2\n",
      "Merkle      False                      24\n",
      "            True                        8\n",
      "dtype: int64\n",
      "Newspaper-based results: \n",
      " company     investing_in_solarparks\n",
      "Enrego      False                       6\n",
      "            True                       18\n",
      "Enviria     False                      18\n",
      "            True                       36\n",
      "HIH Invest  False                      12\n",
      "            True                        3\n",
      "Merkle      False                      21\n",
      "            True                       11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Rule-based results: \\n\", rule_based_results)\n",
    "print(\"SpaCy-based results: \\n\", spacy_results)\n",
    "print(\"Newspaper-based results: \\n\", newspaper_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as seen, using newspaper to gather more details about articles drastically changes the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Based Approach\n",
    "* Computationally consuming\n",
    "* Can give more accurate results and extract more information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
